{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "from abslithist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = lltk.load('CanonFiction')\n",
    "# C.texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_POS=None\n",
    "def get_nlp_pos(force=True):\n",
    "    #global NLP_POS\n",
    "    if not force and NLP_POS is not None: return NLP_POS\n",
    "    import stanza\n",
    "    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma',verbose=False)\n",
    "    if not force: NLP_POS = nlp\n",
    "    return nlp\n",
    "\n",
    "def get_text_pos(path_txt, lim_paras=100, progress=True, nlp_max_chars=3000):\n",
    "    if not os.path.exists(path_txt): return pd.DataFrame()    \n",
    "\n",
    "    import stanza\n",
    "    nlp = stanza.Pipeline('en', processors='tokenize,pos,lemma',verbose=False)\n",
    "\n",
    "    with open(path_txt) as f: text = f.read()\n",
    "    paras = [para.strip() for para in text.split('\\n\\n')]\n",
    "    paras = [' '.join(para.strip().split()) for para in paras if para]\n",
    "    paras = paras[:lim_paras]    \n",
    "    \n",
    "    o=[]\n",
    "    if progress: paras=tqdm(paras,position=0)\n",
    "    for para_i,para in enumerate(paras):\n",
    "        # if len(para)<500: continue\n",
    "        if nlp_max_chars and len(para)>nlp_max_chars:\n",
    "            para_sents = nltk.sent_tokenize(para)\n",
    "            para_parts = []\n",
    "            para_part_len=0\n",
    "            para_part = []\n",
    "            for para_sent in para_sents:\n",
    "                para_part.append(para_sent)\n",
    "                para_part_len+=len(para_sent)\n",
    "                if para_part_len>=nlp_max_chars:\n",
    "                    para_parts.append(' '.join(para_part))\n",
    "                    para_part = []\n",
    "                    para_part_len = 0\n",
    "            if para_part: para_parts.append(' '.join(para_part))\n",
    "        else:\n",
    "            para_parts=[para]\n",
    "\n",
    "\n",
    "        sent_i=0\n",
    "        for parapart_i,para_part in enumerate(para_parts):\n",
    "            # return para_part\n",
    "            doc = nlp(para_part)\n",
    "            for sent in doc.sentences:\n",
    "                sent_i+=1\n",
    "                toks = sent.to_dict()\n",
    "                for tokd in toks:\n",
    "                    tokd['sent_i']=sent_i\n",
    "                    tokd['para_i']=para_i+1\n",
    "                    # tokd['parapart_i']=parapart_i+1\n",
    "                    if 'feats' in tokd:\n",
    "                        for feat in tokd['feats'].split('|'):\n",
    "                            if '=' in feat:\n",
    "                                fk,fv = feat.split('=')\n",
    "                                tokd[fk]=str(fv)\n",
    "                        del tokd['feats']\n",
    "                    o.append(tokd)\n",
    "    return pd.DataFrame(o).rename({'id':'word_i'},axis=1).set_index(['para_i','sent_i','word_i']).fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text_pos(C.t.path_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_save_text_pos(obj,force=False,**kwargs):\n",
    "    path_txt,path_pos = obj\n",
    "    if not force and os.path.exists(path_pos): return\n",
    "    odf = get_text_pos(path_txt, **kwargs)\n",
    "    odir = os.path.dirname(path_pos)\n",
    "    try:\n",
    "        if not os.path.exists(odir):\n",
    "            os.makedirs(odir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    odf.to_pickle(path_pos)\n",
    "\n",
    "def save_text_pos(C,lim=None,num_proc=1,force=False,lim_paras=100,progress_inner=True, nlp_max_chars=3000):\n",
    "    kwargs=dict(progress=progress_inner, lim_paras=lim_paras, nlp_max_chars=nlp_max_chars, force=force)\n",
    "    objs = [(t.path_txt, t.path_pos) for t in C.texts()][:lim]\n",
    "    pmap(do_save_text_pos, objs, num_proc=num_proc, kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C._texts = [t for t in C.texts() if not t.meta.get('corpus_source')]\n",
    "len(C.texts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_text_pos(C, lim_paras=None, force=False, num_proc=4, lim=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99c3488a746bccd77b7949f8f419451e1c962da56ebbaaae6e4d2e6c8c07eaa0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
