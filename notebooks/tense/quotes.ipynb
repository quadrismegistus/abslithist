{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "from abslithist import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = lltk.load('CanonFiction')\n",
    "# C.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0,'/Users/ryan/github/cadence')\n",
    "import cadence as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=\"\"\"Happy families are all alike; every unhappy family is unhappy in its\n",
    "own way.\n",
    "Everything was in confusion in the Oblonskys’ house. The wife had\n",
    "discovered that the husband was carrying on an intrigue with a French\n",
    "girl, who had been a governess in their family, and she had announced\n",
    "to her husband that she could not go on living in the same house with\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install statement-extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statement_extractor import extract_statements\n",
    "# text = 'He said \"I support you.\"'\n",
    "# statements = extract_statements(text)\n",
    "# print(statements)\n",
    "# # prints [{'speaker': \"He\", 'statement': \"I support you.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "\n",
    "PUNCTUATION_TO_REMOVE = ['.', ',', '!', '?']\n",
    "\n",
    "# ATTRIBUTION_WORDS_STEMMED = [u'disse', u'afirmou', u'comentou', u'completou', u'concluiu', u'reforçou', u'admitiu',u'negou', u'apontou']\n",
    "ATTRIBUTION_WORDS_STEMMED = ['say', 'affirm', 'comment', 'complete', 'conclude', 'reinforce', 'admit','deny', 'point']\n",
    "\n",
    "PRONOUNS = ['he', 'she']\n",
    "\n",
    "class Quotation(object):\n",
    "\n",
    "    def __init__(self, phrase, verb):\n",
    "        self.phrase = phrase\n",
    "        self.verb = verb\n",
    "\n",
    "    def contains(self, sentence):\n",
    "        word_present = 0\n",
    "        for word in sentence:\n",
    "            if word in self.phrase:\n",
    "                word_present += 1\n",
    "        return (word_present > len(sentence) - 4)\n",
    "\n",
    "    def __unicode__(self):\n",
    "        return self.phrase\n",
    "\n",
    "\n",
    "def find_quotes(text):\n",
    "    pat = re.compile(r\"\\\"(.*?)\\\",\\s+(\\w+)\", re.UNICODE)\n",
    "    normalized = text.replace(u'\\u201c', '\"')\n",
    "    normalized = normalized.replace(u'\\u201d', '\"')\n",
    "    matches = pat.findall(normalized)\n",
    "    return [Quotation(phrase, verb) for phrase, verb in matches]\n",
    "\n",
    "\n",
    "def annotate_quotes_with_line_numbers(quotes, sentences):\n",
    "    numbered_sentences = list(enumerate(sentences))\n",
    "    for quotation in quotes:\n",
    "        positions = []\n",
    "        for pos, sentence in numbered_sentences:\n",
    "            if quotation.contains(sentence):\n",
    "                positions.append(pos)\n",
    "        quotation.positions = positions\n",
    "\n",
    "\n",
    "def fetch_article(url, language='pt'):\n",
    "    article = newspaper.Article(url, language=language)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article\n",
    "\n",
    "\n",
    "def ner_candidates(tree, symbol):\n",
    "    candidate = []\n",
    "    for production in tree.productions():\n",
    "        if production.is_nonlexical():\n",
    "            continue\n",
    "        if production.lhs().symbol() == symbol:\n",
    "            candidate.append(\" \".join(i[0] for i in production.rhs()))\n",
    "    return candidate\n",
    "\n",
    "\n",
    "def heuristic_elect_candidate(candidates):\n",
    "    names = {}\n",
    "    for pos, candidate_list in candidates:\n",
    "        for candidate in candidate_list:\n",
    "            try:\n",
    "                names[candidate] = names[candidate] + 1\n",
    "            except KeyError:\n",
    "                names[candidate] = 1\n",
    "\n",
    "    for other_name, count in names.copy().items():\n",
    "        for name in names:\n",
    "            if name == other_name:\n",
    "                continue\n",
    "            if other_name in name:\n",
    "                names[name] += count\n",
    "    won = sorted(((v, k) for k, v in names.items()))[-1][1]\n",
    "    return won\n",
    "\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def text_to_sentences(raw_text):\n",
    "    sentences = sentence_tokenizer.tokenize(raw_text)\n",
    "    tokenized_sentences = [word_tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=\"\"\"Bob walked down the street. He said, \"What is going on?\" \"\"\"\n",
    "\n",
    "\n",
    "find_quotes(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_i</th>\n",
       "      <th>word_i</th>\n",
       "      <th>word_str</th>\n",
       "      <th>pos_upos</th>\n",
       "      <th>pos_xpos</th>\n",
       "      <th>pos_degree</th>\n",
       "      <th>pos_number</th>\n",
       "      <th>pos_mood</th>\n",
       "      <th>pos_tense</th>\n",
       "      <th>pos_verbform</th>\n",
       "      <th>pos_person</th>\n",
       "      <th>pos_gender</th>\n",
       "      <th>pos_poss</th>\n",
       "      <th>pos_prontype</th>\n",
       "      <th>pos_definite</th>\n",
       "      <th>pos_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Happy</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pos</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>families</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NNS</td>\n",
       "      <td></td>\n",
       "      <td>Plur</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>are</td>\n",
       "      <td>AUX</td>\n",
       "      <td>VBP</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Ind</td>\n",
       "      <td>Pres</td>\n",
       "      <td>Fin</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>all</td>\n",
       "      <td>ADV</td>\n",
       "      <td>RB</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>alike</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pos</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>DT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Art</td>\n",
       "      <td>Def</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>same</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>Pos</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>house</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NN</td>\n",
       "      <td></td>\n",
       "      <td>Sing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_i  word_i  word_str  ... pos_prontype pos_definite pos_case\n",
       "0        1       1     Happy  ...                                   \n",
       "1        1       2  families  ...                                   \n",
       "2        1       3       are  ...                                   \n",
       "3        1       4       all  ...                                   \n",
       "4        1       5     alike  ...                                   \n",
       "..     ...     ...       ...  ...          ...          ...      ...\n",
       "66       3      41        in  ...                                   \n",
       "67       3      42       the  ...          Art          Def         \n",
       "68       3      43      same  ...                                   \n",
       "69       3      44     house  ...                                   \n",
       "70       3      45      with  ...                                   \n",
       "\n",
       "[71 rows x 16 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd.get_nlp_doc_wordfeat_df(nlp(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_text(path_txt, lim_sents=100):\n",
    "    nlp = cd.get_nlp(pretokenized=False, postag=True, tokenize=True, constituency=False, depparse=False)\n",
    "\n",
    "    with open(path_txt) as f: txt=f.read()\n",
    "    txt=txt.replace('\\n\\n','\\n')\n",
    "    sents = nltk.sent_tokenize(txt)[:lim_sents]\n",
    "    o=[]\n",
    "    for sent_i,sent in enumerate(tqdm(sents)):\n",
    "        sent_doc = nlp(sent)\n",
    "        sent_df = cd.get_nlp_doc_wordfeat_df(sent_doc)\n",
    "        o.append(sent_df.assign(sent_i0=sent_i+1))\n",
    "    return pd.concat(o).fillna('') if len(o) else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.80it/s]\n"
     ]
    }
   ],
   "source": [
    "df=sample_text(C.t.path_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sent_i.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparas_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpara_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mshuffle_paras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreset_paras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlim_paras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Iterating over paragraphs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "    \u001b[0;32mdef\u001b[0m \u001b[0mparas_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpara_i\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mshuffle_paras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreset_paras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mlim_paras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Iterating over paragraphs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mpara_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpara_i_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpara_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mint\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mpara_i_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paras_i\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mreset_paras\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpara_i_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__paras_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mshuffle_paras\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_i_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlim_paras\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpara_i_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpara_i_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlim_paras\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_i_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpara_i_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_i_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpara_i_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/github/cadence/cadence/parsers/docs.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "t.paras_i??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99c3488a746bccd77b7949f8f419451e1c962da56ebbaaae6e4d2e6c8c07eaa0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
